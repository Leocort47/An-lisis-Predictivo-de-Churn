# -*- coding: utf-8 -*-
"""Análisis Predictivo de Churn-Leandro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qcSphY88fhpZawvk-KB8KGsswJIqvBT1
"""

import pandas as pd
import numpy as np
import csv
import matplotlib.pyplot as plt
import seaborn as sns


import warnings, requests, zipfile, io
warnings.simplefilter('ignore')
from scipy.io import arff


#importo los modelos
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split

#importo las métricas

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve,auc
from sklearn.metrics import accuracy_score

#Librerias de validacion cruzada
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict



#Se crea el dataframe y se inserta la información
df=pd.read_csv("https://raw.githubusercontent.com/adiacla/bigdata/master/DatosEmpresaChurn.csv")
df

#se verifica si existen datos nulos
df.isnull().sum()

#se da a conocer el nombre de cada una de las columnas, como podemos apreciar estas se encuentran en ingles
df.columns

#el codigo head se utiliza para tener una muestra de la informacion
df.head(50)

#se realiza un cambio de los nombres de las columnas, se realizo el cambio por comodidad para una mejor manipulación de la información
nuevos_nombres = ['ID', 'Antiguedad', 'Compra', 'Promociones', 'Categoria', 'ComInt', 'ComPres', 'Valoracion', 'Visita', 'Dias_In', 'Tasa_Retencion', 'NumQ', 'Indicador_Retencion', 'Sefue']
df.columns = nuevos_nombres
df

#como se puede apreciar existen datos nulos en las columnas visita y categoria
#para eliminar los nulos de la columna se utilizo el metodo interpolate

df['Visita'] = df['Visita'].interpolate(method='linear')
df['Categoria'] = df['Categoria'].interpolate(method='linear')

#se verifica la informacion quedo correcta
df.isnull().sum()

df

#se borra la columna ID ya que esta no nos va ayudar en nuestro modelo de machine learning
df=df.drop('ID', axis=1)
df

#Todos los datos que se imputaron a las columnas con nulos son todos diferentes entre si
#por lo tanto son variables a descatar en nuestra investigacion
df['Visita'].value_counts()
df['Categoria'].value_counts()

#df.info() nos proporciona un resumen consiso de la informacion del DataFrame
df.info()

df['Compra']=df['Compra'].astype('float64')
df['ComInt']=df['ComInt'].astype('float64')
df['ComPres']=df['ComPres'].astype('float64')
df['Dias_In']=df['Dias_In'].astype('float64')

# Reemplazar comas por puntos en la columna "Antiguedad"
df["Antiguedad"] = df["Antiguedad"].str.replace(',', '.')

# Convertir la columna "Antiguedad" a tipo float
df["Antiguedad"] = df["Antiguedad"].astype('float64')

#se hace un conteo y se grafican los datos existen en la columna "Sefue"
df.Sefue.plot.hist() #graficar en un histograma
pd.value_counts(df.Sefue) #datos

# el comando df.describe() nos proprciona estadisticas descriptivas del DataFrame
df.describe()



# Se grafican las columnas numericas para obtener informacion de ellas
df.plot()

#este codigo realiza la correlacion de cada una de las columnas con la columna "Sefue"
corr_matrix=df.corr()
corr_matrix["Sefue"].sort_values(ascending=False)

#Se realiza un mapa de calor de correlacion
fig, ax=plt.subplots(figsize=(10,10))
colormap=sns.color_palette("BrBG",10)
sns.heatmap(corr_matrix, cmap=colormap, annot=True, fmt=".2f")
plt.show

#Provisionalmente estas son las correlaciones que se utilizan para los modelos
#Tasa_Retencion         0.644892
#Indicador_Retencion    0.553375
#Dias_In               -0.573315
#Promociones           -0.596362
#Categoria             -0.499165



df.head()

#from sklearn.preprocessing import MinMaxScaler
#df['Antiguedad'] = df['Antiguedad'].str.replace(',', '.')

from sklearn.preprocessing import MinMaxScaler

# Instanciar el MinMaxScaler
scaler = MinMaxScaler()

# Escalar la columna "Antiguedad"
df['Antiguedad'] = scaler.fit_transform(df[['Antiguedad']])

df.dtypes

#para verificar lo anterior en se normalizaran los datos
scaler = MinMaxScaler()
df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
df_normalized

#se verifican las correlaciones
corr_matrix=df_normalized.corr()
corr_matrix["Sefue"].sort_values(ascending=False)

df.columns

# Lista de columnas a conservar
columnas_a_conservar = ['Tasa_Retencion', 'Indicador_Retencion', 'Dias_In', 'Promociones', 'Categoria', 'Sefue']

# Eliminar las columnas que no están en la lista de columnas a conservar
df = df[columnas_a_conservar]
df

#Datos de prueba y los datos entrenamiento con un división 80%
X=df.drop("Sefue", axis=1) #selecciono las caracteristicas
y=df["Sefue"]#selecciono la columna target

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=454, stratify=df['Sefue'])

#Declarar la instancia del modelo o Crear el modelo
modeloNB=GaussianNB()

#Entrenar el modelo, fit()
modeloNB.fit(X_train,y_train)

import joblib as jb
jb.dump(modeloNB,"ModeloNB.bin",compress=True)

numeros_aleatorios1 = X.sample(n=1, axis=0)
numeros_aleatorios1

numeros_aleatorios2 = X.sample(n=1, axis=0)
numeros_aleatorios2

numeros_aleatorios3 = X.sample(n=1, axis=0)
numeros_aleatorios3

numeros_aleatorios4 = X.sample(n=1, axis=0)
numeros_aleatorios4

nuevos_pacientes=[[0.890937, 18.306637, 1285.0, 3.27, 0.82, 4980.13],
 [0.911542, 15.279374	, 1266.0	, 3.68, 6115.91],
 [1.041027, 15.239713, 1241.0	, 4.4,  9168.78],
 [0.922683, 25.997692, 728.0	, 3.14, 4.860753, 5615.49]]

for i, paciente in enumerate(nuevos_pacientes):
    print(f"Elemento {i}: {type(paciente)}, {len(paciente)} elementos")

prediccion=modeloNB.predict(nuevos_pacientes)
prediccion

from sklearn.metrics import accuracy_score

predicciones1 = modeloNB.predict(X_test)
precision1 = accuracy_score(y_test, predicciones1)
print(precision1)